#import "utils.typ": *

#show: module-card.with(
  title: [Sampling Strategies],
  subtitle: "My First LM Module #6",
)

Even after your model is trained, you have creative control over how it
generates text.

== You will need

- a completed model from an earlier module

== Your goal

To generate text using the same model, but using at least two different
temperature values and at least two different truncation strategies.

== Key idea

There are lots of different sampling strategies---ways to select the next word
during generation. Each strategy serves a different purpose, from maximising
accuracy to embracing chaos, from creating structured poetry to mimicking child
speech. Understanding these strategies reveals how modern LLMs control their
output style.

#column-section[
== Algorithms (with examples)

There are two ways that a sampling strategy can affect the text generated by
your model:

- *temperature* controls the randomness by adjusting the relative likelihood of
  probable vs improbable words (flattening or sharpening the probability
  distribution)
- *truncation* narrows the viable "next word options" by setting some options
  counts to zero (e.g., top-k, top-p, or constraint-based filtering)

Note: any of the truncation strategies can be combined with temperature control.

=== Temperature control

If the counts in a given row are

- `spot` (4)
- `run` (2)
- `jump` (1)
- `.` (1)

+ *temperature = 1 (normal)*:
  - use counts as-is: 4, 2, 1, 1
  - this means `spot` is 2x as likely as `run`, and 4x as likely as `jump` or
    `.`
+ *temperature = 2 (warmer)*:
  - divide all counts by 2 (round down, min 1): 2, 1, 1, 1
  - `spot` still the most likely, but now only 2x as likely as the others
+ *temperature = 3 (hot)*: divide tallies by 3, round down (min 1)
  - divide all counts by 3 (round down, min 1): 1, 1, 1, 1
  - each word equally likely

Raising the temperature paramter naturally flattens probability differences:

- common words (high tallies) become less dominant
- rare words (low tallies) get relatively more chance
- rounding creates the non-linearity that makes temperature effective
- at high temperatures, everything approaches equal probability

=== Truncation: haiku sampling

Constrain generation to fit the 5-7-5 syllable pattern of haiku poetry. This
creates structured poetry with syllable constraints.

+ track syllables in current line (5-7-5 pattern)
+ roll dice to select next word as normal
+ if selected word exceeds line's syllable limit, re-roll
+ start new line when syllable count reached

=== Truncation: no-repeat sampling

Never use the same word twice in a sentence. This forces vocabulary diversity
and avoids repetitive loops.

+ track all words used in current sentence
+ roll dice to select next word as normal
+ if word already used, re-roll
+ if no valid options remain, insert a `.` and keep going from there

==== Truncation: non-sequitur sampling

Always choose the _least_ likely next word for maximum surprise. Perfect for
surrealist poetry, breaking expectations, and comedy.

+ find current word's row
+ pick the column with the lowest (non-zero) count
+ if there's a tie, roll a dice and choose equally among the least likely
  options

==== Truncation: alliteration sampling

Prefer words starting with the same sound. Perfect for tongue twisters and
memorable phrases.

+ note first letter/sound of previous word
+ if any next-word options start with same letter/sound, sample only from those
  alliterative options
+ otherwise use standard sampling
]
